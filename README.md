# HuMaIN [(humain.acis.ufl.edu)](http://humain.acis.ufl.edu/)
Work on NSF grant ACI-1535086: Human- and Machine-Intelligent Software Elements for Cost-Effective Scientific Data Digitization.<br/>
<br/>
Biodiversity information extraction (IE) from imaged text in digitized museum specimen records is a challenging task due to both the large number of labels and the complexity of the characters and information to be extracted. <br/>
The HuMaIN project investigates software-enabled solutions that support the combination of machine and human intelligence to accelerate IE from specimen labels.<br/>
Among other contributions, the project proposed the use of self-aware workflows to orchestrate machines and human tasks (the SELFIE model), Optical Character Recognition (OCR) ensembles and Natural Language Processing (NLP) methods to increase confidence in extracted text, named-entity recognition (NER) techniques for Darwin Core (DC) terms extraction, and a simulator for the study of these workflows with real-world data. The software has been tested and applied on large datasets from museums in the USA and Australia.<br/>
<br/>

## Acknowledgement
HuMaIN is funded by a grant from the National Science Foundation's ACI Division of Advanced Cyberinfrastructure (Award Number: 1535086).
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and
do not necessarily reflect the views of the National Science Foundation.

## Generalizable Self-aware Information Extraction
**Description**: This study proposes extraction and confidence estimation methods that can be applied to many DC terms in SELFIE workflows. A method to automatically train a named-entity recognition model is utilized for information extraction, while local and global frequency lists are utilized for confidence estimation.<br/>
**Paper by**: Icaro Alzuru, Andréa Matsunaga, Maurício Tsugawa, and José A.B. Fortes. *Generalizable Self-aware Information Extraction from Labels of Biological Collections*. To be submitted. 2020.<br/>
To have access to the scripts utilized in the experiments, see the ***rb_ner.csv*** workflow in the [HuMaIN Simulator repository](https://github.com/acislab/HuMaIN_Simulator).

## Quality-aware Text and Information Extraction
**Description**: The paper *Quality-Aware Human-Machine Text Extraction for Biocollections using Ensembles of OCRs* was extended by including Darwin Core terms extraction experiments that study the benefits of performing terms extraction only from the high-confidence extracted text (generated by the Ensemble of OCR engines).<br/>
**Journal**: Icaro Alzuru, Rhiannon Stephens, Andréa Matsunaga, Maurício Tsugawa, Paul Flemons, and José A.B. Fortes, *Quality-Aware Text and Information Extraction from Biocollections by using Ensembles of OCRs*, Under Review. 2020.<br/>
To have access to the scripts utilized in the experiments, please check the [repository  of the paper](https://github.com/acislab/HuMaIN_Text_Extraction).

## HuMaIN Simulator
**Description**: Conducting research and trying new techniques in the hybrid human-machine IE is resource and time consuming. This paper presents a simulator intended to accelerate research in this area. It includes the simulation engine, human-machine IE workflows for three DC terms, code of the automated IE methods, crowdsourced and ground truth transcriptions of the DC terms of three biocollections, and several experiments that exemplify its use.<br/>
**Paper**: Icaro Alzuru, Aditi Malladi, Andréa Matsunaga, Maurício Tsugawa, and José A.B. Fortes, *Human-Machine Information Extraction Simulator for Biological Collections*, 2019 IEEE International Conference on Big Data (Big Data), Los Angeles, CA, USA, 2019, pp. 4565-4572. [10.1109/BigData47090.2019.9005601](https://doi.org/10.1109/BigData47090.2019.9005601)<br/>
To have access to the scripts utilized in the experiments, please check the [repository of the paper](https://github.com/acislab/HuMaIN_Simulator).

## OCR Ensembles for Text Extraction
**Description**: In this study, an ensemble of OCR engines (OCRopus, Tesseract, and Google Cloud OCR) is used to estimate confidence in the text extracted from specimens' images. Two human-machine crowdsourcing techniques are also tried. In all, the number of crowdsourcing tasks required to transcribed the text is reduced by 76%.<br/>
**Paper**: Icaro Alzuru, Rhiannon Stephens, Andréa Matsunaga, Maurício Tsugawa, Paul Flemons, and José A.B. Fortes, *Quality-Aware Human-Machine Text Extraction for Biocollections using Ensembles of OCRs*, 2019 15th International Conference on eScience (eScience), San Diego, CA, USA, 2019, pp. 116-125. [10.1109/eScience.2019.00020](https://doi.org/10.1109/eScience.2019.00020) <br/>
To have access to the scripts utilized in the experiments, please check the [repository  of the paper](https://github.com/acislab/HuMaIN_Text_Extraction).

## Self-aware Information Extraction (SELFIE) from Biocollections
**Description**: A workflow model (SELFIE) for the extraction of Darwin Core terms from specimens' images is proposed. The approach permits us to evaluate the confidence of extracted values and deciding if another extraction task (e.g., crowdsourcing) should perform the extraction. Three IE implemented experiments showed a reduction of 32% in the required number of humans. The quality was negligibly reduced by 0.27%.<br/>
**Paper**: Icaro Alzuru, Andréa Matsunaga, Maurício Tsugawa, and José A.B. Fortes, *SELFIE: Self-Aware Information Extraction from Digitized Biocollections*, 2017 IEEE 13th International Conference on e-Science (e-Science), Auckland, 2017, pp. 69-78. [doi.org/10.1109/eScience.2017.19](http://doi.org/10.1109/eScience.2017.19)<br/>
To have access to the scripts utilized in the experiments, please check the [repository  of the paper](https://github.com/acislab/HuMaIN_Self-aware_Information_Extraction).

## Task Design and Crowd Sentiment in IE from Biocollections
**Description**: Many Darwin Core terms are extracted by volunteers through online interfaces. These interfaces are very diverse. Using 30 crowdsourcing experiments and two
different populations, the speed, output quality, difficulty, and enjoyability of interfaces are evaluated when changing their data-entry method and  granularity. <br/>
**Paper**: Icaro Alzuru, Andréa Matsunaga, Maurício Tsugawa, and José A.B. Fortes, *Task Design and Crowd Sentiment in Biocollections Information Extraction*, 2017 IEEE 3rd International Conference on Collaboration and Internet Computing (CIC), San Jose, CA, 2017, pp. 389-398. [doi.org/10.1109/CIC.2017.00056](http://doi.org/10.1109/CIC.2017.00056)<br/>
To have access to the scripts utilized in the experiments, please check the [repository  of the paper](https://github.com/acislab/HuMaIN_Crowdsourcing_Complexity).

## Cooperative Human-Machine Data Extraction
**Description**: This study shows how OCRopus and Tesseract, two widely used open-source OCR engines, can improve their accuracy by more than 42%, when text segments are cropped by humans instead of their automated process. Our experiments also reveal differences in speed and quality between Tesseract and OCRopus.<br/>
**Paper**: Icaro Alzuru, Andréa Matsunaga, Maurício Tsugawa, and José A.B. Fortes, *Cooperative human-machine data extraction from biological collections*, 2016 IEEE 12th International Conference on e-Science (e-Science), Baltimore, MD, 2016, pp. 41-50. [doi.org/10.1109/eScience.2016.7870884](http://doi.org/10.1109/eScience.2016.7870884)<br/>
To have access to the scripts utilized in the experiments, please check the [repository  of the paper](https://github.com/acislab/HuMaIN_Collaborative_Data_Extraction).
<br/>
<br/>
<br/>
<br/>
HuMaIN is funded by a grant from the National Science Foundation's ACI Division of Advanced Cyberinfrastructure (Award Number: 1535086). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.<br/>
